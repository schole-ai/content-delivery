{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import ollama\n",
    "import time\n",
    "import requests\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm \n",
    "from neo4j import GraphDatabase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBpedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URI = \"bolt://localhost:7687\" \n",
    "USERNAME = \"neo4j\"  \n",
    "PASSWORD = \"password123\" \n",
    "\n",
    "driver = GraphDatabase.driver(URI, auth=(USERNAME, PASSWORD))\n",
    "print(\"Connected to Neo4j!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT TURTLE FILE\n",
    "IMPORT_FOLDER = r\"C:/Users/ylaar/.Neo4jDesktop/relate-data/dbmss/dbms-ce0521ea-df1c-40e3-8b68-94d8496673a1/import\"\n",
    "IMPORTED_FILES_LIST = \"imported_files.txt\"\n",
    "\n",
    "# load the list of already imported files\n",
    "if os.path.exists(IMPORTED_FILES_LIST):\n",
    "    with open(IMPORTED_FILES_LIST, \"r\", encoding=\"utf-8\") as f:\n",
    "        imported_files = set(f.read().splitlines()) \n",
    "else:\n",
    "    imported_files = set()\n",
    "\n",
    "turtle_files = [f for f in os.listdir(IMPORT_FOLDER) if f.endswith('.ttl')]\n",
    "turtle_files_to_import = [f for f in turtle_files if f not in imported_files]\n",
    "\n",
    "def import_ttl_file(tx, file_path):\n",
    "    file_path = file_path.replace(\"\\\\\", \"/\")  # Fix Windows backslashes\n",
    "    cypher_query = (\n",
    "        f'CALL n10s.rdf.import.fetch(\"file:///{file_path}\", \"Turtle\", '\n",
    "        '{ handleVocabUris: \"SHORTEN\" });'\n",
    "    )\n",
    "    tx.run(cypher_query)\n",
    "\n",
    "with driver.session() as session:\n",
    "    with tqdm(total=len(turtle_files_to_import), desc=\"Importing Turtle Files\", unit=\"file\") as pbar:\n",
    "        for file in turtle_files_to_import:\n",
    "            file_path = os.path.join(IMPORT_FOLDER, file)\n",
    "            print(f\"Processing: {file}\") \n",
    "            session.execute_write(import_ttl_file, file_path)\n",
    "            with open(IMPORTED_FILES_LIST, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(file + \"\\n\")\n",
    "\n",
    "            pbar.update(1)  # Update progress bar\n",
    "\n",
    "driver.close()\n",
    "print(\"All TTL files imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/CSO/CSO.3.4.1.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df_clean = df.copy()\n",
    "df_clean.columns = ['topic1', 'relationship', 'topic2']\n",
    "\n",
    "# Function to extract the topic name from the URL and clean special characters\n",
    "def extract_topic(url):\n",
    "    if pd.notna(url): \n",
    "        topic = url.split('/')[-1].replace('>', '').replace('<', '')\n",
    "        topic = topic.split('_%')[0]  # Remove special character part like _%28svms%29\n",
    "        return topic.split('%')[0]  # Also remove any other encoded characters like %2C\n",
    "    return url\n",
    "\n",
    "df_clean['topic1'] = df_clean['topic1'].apply(extract_topic)\n",
    "df_clean['topic2'] = df_clean['topic2'].apply(extract_topic)\n",
    "\n",
    "# Clean up the relationship column\n",
    "df_clean['relationship'] = df_clean['relationship'].apply(\n",
    "    lambda x: (x.split('#')[-1] if '#' in x else x.split('/')[-1]).replace('>', '').replace('<', '') if pd.notna(x) else x\n",
    ")\n",
    "\n",
    "df_clean.drop_duplicates(inplace=True)\n",
    "\n",
    "\n",
    "clean_file_path = 'knowledge_graph/CSO_cleaned.csv'\n",
    "df_clean.to_csv(clean_file_path, index=False)\n",
    "\n",
    "print(f\"Cleaned file saved at: {clean_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate content: Option 1 - with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"CSO/CSO_cleaned.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "def is_valid_topic(topic):\n",
    "    return bool(re.match(r\"^[a-zA-Z][a-zA-Z0-9_\\- ]*$\", str(topic)))\n",
    "\n",
    "# Extract unique topics and relationships\n",
    "unique_topics = set(df[\"topic1\"].dropna()).union(set(df[\"topic2\"].dropna()))\n",
    "cleaned_topics = [topic for topic in unique_topics if is_valid_topic(topic)]\n",
    "\n",
    "# Build ontology mapping (topic -> parent topic)\n",
    "ontology = defaultdict(set)\n",
    "for _, row in df.dropna().iterrows():\n",
    "    ontology[row[\"topic2\"]].add(row[\"topic1\"])  # topic2 is a sub-topic of topic1\n",
    "\n",
    "# Function to generate a content for a given topic\n",
    "def generate_description(topic):\n",
    "    topic_formatted = topic.replace(\"_\", \" \")\n",
    "    parent_topics = ontology.get(topic, [])\n",
    "    parent_list = \", \".join([p.replace(\"_\", \" \") for p in parent_topics]) if parent_topics else \"computer science\"\n",
    "\n",
    "    prompt = f\"\"\"Generate an educational description for the topic: '{topic_formatted}'. \n",
    "    It belongs to the category: {parent_list}. \n",
    "    The description should be informative, structured, and suitable for students and researchers. \n",
    "    Include definitions, use cases, and why it's important.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = ollama.chat(model=\"mistral\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "        return response[\"message\"][\"content\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating content for {topic}: {e}\")\n",
    "        return f\"{topic_formatted} is an important topic in {parent_list}.\"\n",
    "\n",
    "# Generate descriptions\n",
    "topic_descriptions = {}\n",
    "for idx, topic in enumerate(cleaned_topics):\n",
    "    print(f\"Generating content for topic {idx+1}/{len(cleaned_topics)}: {topic}\")\n",
    "    topic_descriptions[topic] = generate_description(topic)\n",
    "    time.sleep(1)  # To avoid API rate limits\n",
    "\n",
    "# Convert to DataFrame and save to CSV\n",
    "df_descriptions = pd.DataFrame(list(topic_descriptions.items()), columns=[\"Topic\", \"Description\"])\n",
    "df_descriptions.to_csv(\"CSO_enhanced.csv\", index=False)\n",
    "\n",
    "print(\"Descriptions generated and saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 2: Use wikipedia articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the knowledge graph CSV\n",
    "csv_path = 'CSO/CSO_cleaned.csv'\n",
    "data = pd.read_csv(csv_path)\n",
    "\n",
    "def get_wikipedia_content(title):\n",
    "    \"\"\"Fetch Wikipedia page content for a given title with retry and delay.\"\"\"\n",
    "    url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"titles\": title,\n",
    "        \"prop\": \"extracts\",\n",
    "        \"explaintext\": True,\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    for attempt in range(5):\n",
    "        try:\n",
    "            response = requests.get(url, params=params, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            pages = data.get(\"query\", {}).get(\"pages\", {})\n",
    "            if not pages:\n",
    "                return None\n",
    "            page = next(iter(pages.values()), {})\n",
    "            if \"missing\" in page:\n",
    "                return None\n",
    "            return page.get(\"extract\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Attempt {attempt + 1} failed for {title}: {e}\")\n",
    "            time.sleep(2 ** attempt)\n",
    "    return None\n",
    "\n",
    "def node_exists(tx, node_name):\n",
    "    \"\"\"Check if a node exists.\"\"\"\n",
    "    result = tx.run(\"\"\"\n",
    "        MATCH (n {name: $name})\n",
    "        RETURN COUNT(n) > 0 AS exists\n",
    "    \"\"\", name=node_name)\n",
    "    record = result.single()\n",
    "    return record and record[\"exists\"]\n",
    "\n",
    "def node_has_content(tx, node_name):\n",
    "    \"\"\"Check if a node already has content.\"\"\"\n",
    "    result = tx.run(\"\"\"\n",
    "        MATCH (n {name: $name})\n",
    "        RETURN n.content IS NOT NULL AS hasContent\n",
    "    \"\"\", name=node_name)\n",
    "    record = result.single()\n",
    "    return record and record[\"hasContent\"]\n",
    "\n",
    "def delete_node(tx, node_name):\n",
    "    \"\"\"Delete a node and its relationships.\"\"\"\n",
    "    tx.run(\"\"\"\n",
    "        MATCH (n {name: $name})\n",
    "        DETACH DELETE n\n",
    "    \"\"\", name=node_name)\n",
    "\n",
    "def update_node_content(tx, node_name, content):\n",
    "    \"\"\"Update a node with Wikipedia content.\"\"\"\n",
    "    tx.run(\"\"\"\n",
    "        MATCH (n {name: $name})\n",
    "        SET n.content = $content\n",
    "    \"\"\", name=node_name, content=content)\n",
    "\n",
    "with driver.session() as session:\n",
    "    topics = pd.concat([data['topic1'], data['topic2']]).unique()\n",
    "    for topic in tqdm(topics, desc=\"Processing topics\"):\n",
    "        if not session.execute_read(node_exists, topic):\n",
    "            # print(f\"Skipping: {topic} (already deleted)\")\n",
    "            continue\n",
    "        if session.execute_read(node_has_content, topic):\n",
    "            # print(f\"Skipping: {topic} (already has content)\")\n",
    "            continue\n",
    "        content = get_wikipedia_content(topic)\n",
    "        if content:\n",
    "            session.execute_write(update_node_content, topic, content)\n",
    "            # print(f\"Updated: {topic}\")\n",
    "        else:\n",
    "            session.execute_write(delete_node, topic)\n",
    "            # print(f\"Deleted: {topic}\")\n",
    "        time.sleep(1)\n",
    "\n",
    "driver.close()\n",
    "print(\"Graph update complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4ed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
